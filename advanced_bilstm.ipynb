{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "dynet_config.set(mem='11000', autobatch=1, requested_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "from baseline_load_data import load_questions, VOCAB_SIZE\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def parameters(*params):\n",
    "    yield tuple(map(lambda x:dy.parameter(x), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_questions('./data/train.tok.json')\n",
    "dev_set = load_questions('./data/dev.tok.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.ParameterCollection()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 256\n",
    "ATTENTION_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = model.add_lookup_parameters((VOCAB_SIZE, EMBED_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeRNN = dy.BiRNNBuilder(NUM_LAYERS, EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "# h, q refer to Figure 3 on DeepMind's paper. We don't need r, because we always put intermedia into memory\n",
    "hRNN = dy.LSTMBuilder(NUM_LAYERS, HIDDEN_SIZE + HIDDEN_SIZE + EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "qRNN = dy.LSTMBuilder(NUM_LAYERS, HIDDEN_SIZE + EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "h_init_input = model.add_parameters((HIDDEN_SIZE + HIDDEN_SIZE + EMBED_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention function\n",
    "att_W_ctx = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "att_W_h = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "att_b = model.add_parameters((1, ATTENTION_SIZE))\n",
    "\n",
    "def calc_attention(ctx_matrix, ctx_att, h):\n",
    "    with parameters(att_W_h, att_b) as (W, b):\n",
    "        att_score = dy.transpose(b * dy.tanh(dy.colwise_add(ctx_att, W * h)))\n",
    "        att_p = dy.softmax(att_score)\n",
    "        ctx_mixture = ctx_matrix * att_p\n",
    "        return ctx_mixture, att_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of each operation\n",
    "NUM_OPERATIONS = len(operations_list)\n",
    "opt_embeds = model.add_lookup_parameters((NUM_OPERATIONS, EMBED_SIZE))\n",
    "opt_args = [argsNum[func] for func in operations_list]\n",
    "opt_W = model.add_parameters((NUM_OPERATIONS, HIDDEN_SIZE))\n",
    "opt_b = model.add_parameters((NUM_OPERATIONS))\n",
    "\n",
    "def operation_softmax(h, opt_id):\n",
    "    with parameters(opt_W, opt_b) as (W, b):\n",
    "        probs = dy.softmax(W * h + b)\n",
    "        return probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of each source for an argument\n",
    "NUM_ARGS_SOURCE = 3 # INPUT, MEM, VOC\n",
    "INPUT_SOURCE = 0\n",
    "MEMORY_SOURCE = 1\n",
    "VOC_SOURCE = 2\n",
    "arg_W = model.add_parameters((NUM_ARGS_SOURCE, HIDDEN_SIZE))\n",
    "arg_b = model.add_parameters((NUM_ARGS_SOURCE))\n",
    "\n",
    "def argsource_softmax(h):\n",
    "    with parameters(arg_W, arg_b) as (W, b):\n",
    "        probs = dy.softmax(W * h + b)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the softmax for copy-from-input\n",
    "# Note from the paper: \n",
    "#      apply a linear project from [uij, qij] into a fixed size vector\n",
    "#      followed by a tanh and a linear projection into a single value\n",
    "FIXED_SIZE = 128\n",
    "proj_input_W = model.add_parameters((FIXED_SIZE, HIDDEN_SIZE + EMBED_SIZE))\n",
    "proj_input_b = model.add_parameters((FIXED_SIZE))\n",
    "proj_input_SW = model.add_parameters((1, FIXED_SIZE))\n",
    "proj_input_Sb = model.add_parameters((1))\n",
    "\n",
    "def copyinput_softmax(x, ctx_seq, q):\n",
    "    with parameters(proj_input_W, proj_input_b, proj_input_SW, proj_input_Sb) as (proj_W, proj_b, proj_SW, proj_Sb):\n",
    "        scores = []\n",
    "        for idx, _ in enumerate(x):\n",
    "            val = proj_SW * dy.tanh(proj_W * dy.concatenate([ctx_seq[idx], q]) + proj_b) + proj_Sb;\n",
    "            scores.append(val)\n",
    "        scores_tensor = dy.inputTensor(scores)\n",
    "        props = dy.softmax(scores_tensor)    \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the softmax for copy-from-memory\n",
    "mem_key = []\n",
    "mem_dict = {}\n",
    "FIXED_SIZE = 128\n",
    "\n",
    "proj_mem_W = model.add_parameters((FIXED_SIZE, HIDDEN_SIZE + EMBED_SIZE))\n",
    "proj_mem_b = model.add_parameters((FIXED_SIZE))\n",
    "proj_mem_SW = model.add_parameters((1, FIXED_SIZE))\n",
    "proj_mem_Sb = model.add_parameters((1))\n",
    "\n",
    "def copymem_softmax(q):\n",
    "    with parameters(proj_mem_W, proj_mem_b, proj_mem_SW, proj_mem_Sb) as (proj_W, proj_b, proj_SW, proj_Sb):\n",
    "        scores = []\n",
    "        for idx, key in enumerate(mem_key):\n",
    "            val = proj_SW * dy.tanh(proj_W * dy.concatenate([mem_dict[idx], q]) + proj_b) + proj_Sb;\n",
    "            scores.append(val)\n",
    "        scores_tensor = dy.inputTensor(scores)\n",
    "        props = dy.softmax(scores_tensor)    \n",
    "        return probs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the softmax for copy-from-vocb\n",
    "vocbsf_W = model.add_parameters((HIDDEN_SIZE, VOCAB_SIZE))\n",
    "vocbsf_b = model.add_parameters((VOCAB_SIZE))\n",
    "\n",
    "def copyvocb_softmax(q):\n",
    "    with parameters(vocbsf_W, vocbsf_b) as (vocb_W, vocb_b):\n",
    "        props = dy.softmax(q * vocb_W + vocb_b)\n",
    "        return probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(ori_x, y):\n",
    "    x = [embeds[tid] for tid in x]\n",
    "    ctx_seq = fwRNN.transduce(x)\n",
    "    ctx_matrix = dy.concatenate_cols(ctx_seq)\n",
    "    with parameters(att_W_ctx, h_init_input) as (W, init_input):\n",
    "        ctx_att = W * ctx_matrix\n",
    "        current_h_state = hRNN.initial_state().add_input(init_input)\n",
    "    h = current_h_state.output()\n",
    "    losses = []\n",
    "    \n",
    "    # group = (opt_id, x1, x2, ..., xk, v)\n",
    "    for group in y:\n",
    "        # select operation\n",
    "        opt_id = group[0]\n",
    "        probs = operation_softmax(h, opt_id)\n",
    "        loss = dy.pickneglogsoftmax(probs, opt_id)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # select arguments for the operation\n",
    "        # Here we need to use h to initilize qRNN for the argument generation\n",
    "        current_q_state = qRNN.initial_state().add_input(dy.concatenate([h, opt_embeds[opt_id]]))\n",
    "        q = current_q_state.output()\n",
    "        for idx, cur_arg in enumerate(group[1:-1]):            \n",
    "            # for each arguments, there are multiple source.\n",
    "            q = current_q_state.output()\n",
    "            src_probs = argsource_softmax(q)\n",
    "            \n",
    "            # for each source, we calculate the probability of the target argument\n",
    "            for src in range(NUM_ARGS_SOURCE):\n",
    "                cur_arg_id = tok2id[str(cur_arg)]\n",
    "                # Arg from input\n",
    "                if src == INPUT_SOURCE and cur_arg_id in ori_x:\n",
    "                    cur_arg_oriIdx = ori_x.index(cur_arg_id)\n",
    "                    props = copyinput_softmax(x, ctx_seq, q)\n",
    "                    src_loss = src_probs[src] * dy.pickneglogsoftmax(probs, cur_arg_oriIdx)\n",
    "                    losses.append(src_loss)\n",
    "                # Arg from memory\n",
    "                elif src == MEMORY_SOURCE and cur_arg in mem_dict:\n",
    "                    cur_arg_memIdx = mem_key.index(cur_arg)\n",
    "                    props = copymem_softmax(q)\n",
    "                    src_loss = src_probs[src] * dy.pickneglogsoftmax(probs, cur_arg_memIdx)\n",
    "                    losses.append(src_loss)\n",
    "                # Arg from Voc\n",
    "                else:\n",
    "                    probs = copyvocb_softmax(q)\n",
    "                    src_loss = src_probs[src] * dy.pickneglogsoftmax(probs, cur_arg_id)\n",
    "                    losses.append(src_loss)\n",
    "                \n",
    "                # Update q state from the next argument\n",
    "                current_q_state = current_q_state.add_input(dy.concatenate([q, embeds[cur_arg_id]]))\n",
    "            \n",
    "        # we need to put intermedia into memory\n",
    "        v = group[-1]\n",
    "        mem_key.append(v)\n",
    "        mem_dict[v] = h\n",
    "    \n",
    "        # Update h state from the next group\n",
    "        v_embed = embeds[tok2id[str(v)]]\n",
    "        ctx_mixture, _ = calc_attention(ctx_matrix, ctx_att, h)\n",
    "        current_h_state = current_h_state.add_input(dy.concatenate([ctx_mixture, q, v_embed]))\n",
    "        h = current_h_state.output()\n",
    "    \n",
    "    return dy.esum(losses)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
