{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "dynet_config.set(mem='11000', autobatch=1, requested_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "from baseline_load_data import load_questions, VOCAB_SIZE\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def parameters(*params):\n",
    "    yield tuple(map(lambda x:dy.parameter(x), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_questions('./data/train.tok.json')\n",
    "dev_set = load_questions('./data/dev.tok.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dy.ParameterCollection()\n",
    "trainer = dy.AdamTrainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 256\n",
    "ATTENTION_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = model.add_lookup_parameters((VOCAB_SIZE, EMBED_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwRNN = dy.BiRNNBuilder(NUM_LAYERS, EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "bwRNN = dy.LSTMBuilder(NUM_LAYERS, HIDDEN_SIZE + EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
    "bw_init_input = model.add_parameters((HIDDEN_SIZE + EMBED_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_W_ctx = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "att_W_h = model.add_parameters((ATTENTION_SIZE, HIDDEN_SIZE))\n",
    "att_b = model.add_parameters((1, ATTENTION_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_attention(ctx_matrix, ctx_att, h):\n",
    "    with parameters(att_W_h, att_b) as (W, b):\n",
    "        att_score = dy.transpose(b * dy.tanh(dy.colwise_add(ctx_att, W * h)))\n",
    "        att_p = dy.softmax(att_score)\n",
    "        ctx_mixture = ctx_matrix * att_p\n",
    "        return ctx_mixture, att_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_W = model.add_parameters((VOCAB_SIZE, HIDDEN_SIZE))\n",
    "out_b = model.add_parameters((VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y):\n",
    "    x = [embeds[tid] for tid in x]\n",
    "    ctx_seq = fwRNN.transduce(x)\n",
    "    ctx_matrix = dy.concatenate_cols(ctx_seq)\n",
    "    with parameters(att_W_ctx, bw_init_input) as (W, init_input):\n",
    "        ctx_att = W * ctx_matrix\n",
    "        current_state = bwRNN.initial_state().add_input(init_input)\n",
    "    h = current_state.output()\n",
    "    prev_tid = y[0]\n",
    "    losses = []\n",
    "    with parameters(out_W, out_b) as (W, b):\n",
    "        for next_tid in y[1:]:\n",
    "            ctx_mixture, _ = calc_attention(ctx_matrix, ctx_att, h)\n",
    "            current_state = current_state.add_input(dy.concatenate([ctx_mixture, embeds[prev_tid]]))\n",
    "            h = current_state.output()\n",
    "            probs = dy.softmax(W * h + b)\n",
    "            losses.append(dy.pickneglogsoftmax(probs, next_tid))\n",
    "            prev_tid = next_tid\n",
    "    return dy.esum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('./baseline_bilstm.model'):\n",
    "    model.populate('./baseline_bilstm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_loss(batch_size):\n",
    "    total_loss = 0.0\n",
    "    token_count = 0\n",
    "    for pos in range(0, len(dev_set), batch_size):\n",
    "        dy.renew_cg()\n",
    "        current_batch = dev_set[pos:pos+BATCH_SIZE]\n",
    "        batch_loss = dy.esum([loss(x, y) for x, y in current_batch]) / len(current_batch)\n",
    "        total_loss += batch_loss.value()\n",
    "        token_count += sum(map(len, current_batch))\n",
    "    print('dev perplexity: %f' % (total_loss / token_count))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_loss = None\n",
    "for epoch in itertools.count(1):\n",
    "    print('runing epoch %d...' % epoch)\n",
    "    random.shuffle(train_set)\n",
    "    for num_batch, pos in enumerate(range(0, len(train_set), BATCH_SIZE)):\n",
    "        if num_batch % 500 == 0:\n",
    "            print(time.ctime())\n",
    "            total_loss = dev_loss(BATCH_SIZE)\n",
    "            print('epoch %d batch %d finished' % (epoch, num_batch))\n",
    "            if last_loss is not None and last_loss < total_loss:\n",
    "                print('training stoped due to loss increasing on dev.')\n",
    "                exit(0)\n",
    "            model.save('./baseline_bilstm.model')\n",
    "            last_loss = total_loss\n",
    "            print(time.ctime())\n",
    "        dy.renew_cg()\n",
    "        current_batch = train_set[pos:pos+BATCH_SIZE]\n",
    "        batch_loss = dy.esum([loss(x, y) for x, y in current_batch]) / len(current_batch)\n",
    "        batch_loss.backward()\n",
    "        trainer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import load_vocabs, CHECK_A, CHECK_B, CHECK_C, CHECK_D, CHECK_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_questions('./data/test.tok.json')\n",
    "tok2id, _ = load_vocabs()\n",
    "terminal_tids = set([tok2id[CHECK_A], tok2id[CHECK_B], tok2id[CHECK_C], tok2id[CHECK_D], tok2id[CHECK_E]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, y):\n",
    "    x = [embeds[tid] for tid in x]\n",
    "    ctx_seq = fwRNN.transduce(x)\n",
    "    ctx_matrix = dy.concatenate_cols(ctx_seq)\n",
    "    with parameters(att_W_ctx, bw_init_input) as (W, init_input):\n",
    "        ctx_att = W * ctx_matrix\n",
    "        current_state = bwRNN.initial_state().add_input(init_input)\n",
    "    h = current_state.output()\n",
    "    prev_tid = y[0]\n",
    "    losses = []\n",
    "    with parameters(out_W, out_b) as (W, b):\n",
    "        for _ in range(len(y) * 2):\n",
    "            ctx_mixture, _ = calc_attention(ctx_matrix, ctx_att, h)\n",
    "            current_state = current_state.add_input(dy.concatenate([ctx_mixture, embeds[prev_tid]]))\n",
    "            h = current_state.output()\n",
    "            probs = dy.softmax(W * h + b).npvalue()[:,0]\n",
    "            probs /= probs.sum()\n",
    "            next_tid = np.random.choice(VOCAB_SIZE, 1, p=probs)[0]\n",
    "            if next_tid in terminal_tids:\n",
    "                return next_tid\n",
    "            prev_tid = next_tid\n",
    "    options = list(terminal_tids)\n",
    "    option_probs = np.array([probs[tid] for tid in options])\n",
    "    option_probs /= option_probs.sum()\n",
    "    return np.random.choice(options, 1, p=option_probs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for x, y in test_set[:100]:\n",
    "    counter = {tid: 0 for tid in terminal_tids}\n",
    "    for _ in range(100):\n",
    "        dy.renew_cg()\n",
    "        counter[sample(x, y)] += 1\n",
    "    for tid, count in counter.items():\n",
    "        if tid != y[-1] and counter[tid] > counter[y[-1]]:\n",
    "            break\n",
    "    else:\n",
    "        correct += 1\n",
    "print(float(correct)/len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
